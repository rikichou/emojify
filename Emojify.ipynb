{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "æˆ‘ä»¬éƒ½æœ‰è¿‡ç½‘ä¸Šè´­ç‰©çš„ç»éªŒï¼Œä¸€èˆ¬åœ¨ç¡®è®¤æ”¶è´§çš„æ—¶å€™ç½‘ç«™éƒ½ä¼šè®©æˆ‘ä»¬ä¸ºå•†å“ç•™ä¸‹è¯„è®ºå¹¶ä¸”æ‰“æ˜Ÿæ˜Ÿï¼Œä¸€èˆ¬æ˜¯1æ˜Ÿåˆ°5æ˜Ÿï¼Œæ˜Ÿçº§çš„å¤šå°‘ä¹Ÿä»£è¡¨äº†æˆ‘ä»¬è¿™æ¬¡è´­ç‰©çš„æ»¡æ„ç¨‹åº¦ã€‚é‚£æˆ‘ä»¬èƒ½ä¸èƒ½æ ¹æ®è¯„è®ºçš„å†…å®¹æ¥æ¨æ–­å®¢æˆ·æœ¬æ¬¡è´­ç‰©çš„æ»¡æ„ç¨‹åº¦å‘¢ï¼Ÿäººå½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œä½†æ˜¯ä»Šå¤©æˆ‘ä»¬è¦è®©ç®—æ³•æ¥åšç±»ä¼¼çš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojify\n",
    "\n",
    "ç°åœ¨ï¼Œæœ‰è®¸å¤šè„šæœ¬å¯ä»¥åœ¨æˆ‘ä»¬çš„å†…å®¹ä¸­è‡ªåŠ¨æ’å…¥ä»£è¡¨è¯­ä¹‰çš„è¡¨æƒ…ç¬¦å·ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¥è‹±æ–‡ï¼Œ\"Congratulations on the promotion! Lets get coffee and talk. Love you!\" è¾“å…¥ç›¸åº”çš„è„šæœ¬å°±å¯ä»¥å°†å…¶è½¬æ¢ä¸ºï¼š \"Congratulations on the promotion! ğŸ‘ Lets get coffee and talk. â˜•ï¸ Love you! â¤ï¸\"ã€‚ä½†æ˜¯è¿™äº›è„šæœ¬ä»…ä»…æ˜¯æ ¹æ®å…³é”®å­—çš„è®°å¿†ä»¥åŠåŒ¹é…æ¥å®Œæˆçš„ï¼Œæ— æ³•çœŸæ­£åšåˆ°è¯­ä¹‰çš„ç†è§£ï¼Œè€Œä¸”æ³›åŒ–èƒ½åŠ›ä¸å¼ºï¼Œå¦‚æœæŸä¸ªå…³é”®å­—ä¸å¤„äºè„šæœ¬å†…çš„å…³é”®å­—åˆ—è¡¨å†…çš„è¯å°±æ— æ³•åŒ¹é…ã€‚æ‰€ä»¥æœ¬é¡¹ç›®å°†ä½¿ç”¨RNN+LSTM+Word embeddingæ¥å®Œæˆï¼Œå½“ç„¶ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬è¿˜ä¼šä½¿ç”¨ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆæ¥ä½œä¸ºå¯¹ç…§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -ç®€å•çš„æ–¹æ¡ˆ--softmax regression\n",
    "\n",
    "### 1.1 - æå–æ•°æ®é›†\n",
    "\n",
    "æˆ‘ä»¬çš„æ•°æ®é›†ç”±127ä¸ªè®­ç»ƒæ ·æœ¬å’Œ56ä¸ªæµ‹è¯•æ ·æœ¬ç»„æˆï¼ˆæ ·æœ¬æœ‰ç‚¹å„¿å°‘å“ˆï¼Œä½†æ˜¯è¿™æ ·æ‰èƒ½ä½“ç°å‡ºWord embeddingçš„å¨åŠ›ï¼‰ã€‚\n",
    "\n",
    "å…¶ä¸­Yæ˜¯èŒƒå›´ä¸º0--4çš„æ•´æ•°ï¼Œä»£è¡¨äº†æ¯ä¸ªå¥å­å¯¹åº”çš„è¡¨æƒ…ç¬¦å·ã€‚\n",
    "\n",
    "ä¸‹å›¾ç»™äº†éƒ¨åˆ†çš„æ ·æœ¬ï¼Œé€šè¿‡è§‚å¯Ÿå¯ä»¥å¤§è‡´äº†è§£æ ·æœ¬çš„æƒ…å†µã€‚\n",
    "\n",
    "<img src=\"data_set.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('train_emoji.csv')\n",
    "X_test, Y_test = read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢åšä¸€ä¸‹ç®€å•çš„æ•°æ®å¯è§†åŒ–ï¼Œindexä»£è¡¨æƒ³è¦æŸ¥çœ‹çš„æ ·æœ¬çš„indexï¼Œä¸‹é¢çš„å¥å­å°†ä¼šæ‰“å°å‡ºæ ·æœ¬å¥å­ä¸å…¶å¯¹åº”çš„è¡¨æƒ…ç¬¦å·ï¼ˆå…¶ä¸­è¡¨æƒ…ç¬¦å·ç”¨åˆ°äº†emojiåº“ï¼Œè¯¥åº“æä¾›äº†ä¸°å¯Œçš„è¡¨æƒ…ç¬¦å·ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è‡ªå·±è¯•è¯•[emoji](https://pypi.org/project/emoji/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go play âš¾\n"
     ]
    }
   ],
   "source": [
    "index = 9\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - æ¨¡å‹\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å°±æ¥å®ç°ä¸‹å›¾ä¸­çš„è¿™ä¸ªæ¨¡å‹\n",
    "\n",
    "<img src=\"image_1.png\" style=\"width:900px;height:300px;\">\n",
    "\n",
    "è¯¥æ¨¡å‹çš„è¾“å…¥ä¸ºå¥å­çš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬è§£æå‡ºæ¯ä¸ªå­—ç¬¦ä¸²çš„å•è¯ï¼Œç„¶åå°†å…¶ç”¨è¯å‘é‡è¡¨ç¤ºï¼ˆæ¯”å¦‚50ç»´åº¦çš„è¯å‘é‡ï¼‰ï¼Œç„¶åå°†æ‰€æœ‰çš„è¯å‘é‡å–å¹³å‡ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œè¿™æ ·ï¼Œä¸ç®¡å¥å­çš„é•¿çŸ­ï¼Œéƒ½å¯ä»¥æ­£ç¡®è¾“å…¥æ¨¡å‹ã€‚ç„¶åé€šè¿‡ä¸€ä¸ª5ä¸ªéšè—å•å…ƒçš„softmaxæ¥è¿›è¡Œåˆ†ç±»è¾“å‡ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨argmaxæ¥åˆ¤æ–­å“ªä¸ªè¡¨æƒ…çš„å¯èƒ½æ€§æœ€å¤§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†åŒ¹é…è¾“å‡ºçš„ç»´åº¦ï¼Œæˆ‘ä»¬è¦å°†æ ·æœ¬ä¸­çš„æ ‡ç­¾ä»¥one-hot vectorçš„å½¢å¼è¡¨ç°å‡ºæ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - æ¨¡å‹æ­å»ºä»¥åŠè®­ç»ƒ\n",
    "\n",
    "ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºæ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†è¾“å…¥çš„å¥å­ä»¥è¯å‘é‡çš„å½¢å¼è¡¨ç¤ºï¼Œç„¶åå†å¯¹ä»–ä»¬è¿›è¡Œå¹³å‡ã€‚æ™®é€šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨vocabularyï¼ˆè¯æ±‡è¡¨ï¼‰ç„¶åå†åŠ ä¸Šone-hot representationæ¥è¡¨ç¤ºï¼Œè¿™ç§è¡¨ç¤ºæ–¹å¼çš„ä¸€å¤§ç¼ºç‚¹å°±æ˜¯ï¼Œå®ƒæŠŠæ¯ä¸ªè¯éƒ½å­¤ç«‹èµ·æ¥ï¼Œè¿™æ ·ä½¿å¾—ç®—æ³•å¯¹ç›¸å…³è¯çš„æ³›åŒ–èƒ½åŠ›ä¸å¼ºï¼Œæ¯”å¦‚æ¨¡å‹çŸ¥é“orange juiceï¼Œä½†æ˜¯å¾ˆæœ‰å¯èƒ½æ— æ³•å°†å…¶æ³›åŒ–ä¸ºapple juiceï¼ˆå¦‚æœè®­ç»ƒé›†ä¸­æ²¡æœ‰apple juiceçš„è¯ï¼‰ã€‚ç”±äºä¸¤ä¸ªè¯è¯­çš„one-hot vectorçš„ä¹˜ç§¯ä¸º0ï¼Œæ‰€ä»¥å…¶æœ¬èº«æ— æ³•è¡¨ç¤ºä¸¤ä¸ªè¯è¯­çš„ä»»ä½•å…³ç³»ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œé‡‡ç”¨é¢„è®­ç»ƒçš„Word embeddingï¼ŒWord embeddingæ˜¯ç»è¿‡åºå¤§çš„æ•°æ®é›†è®­ç»ƒï¼ˆ10äº¿ç”šè‡³100äº¿çº§åˆ«ï¼‰çš„è¯å‘é‡ï¼Œå…¶èƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨è¾¾å•è¯ä¹‹é—´çš„ç›¸å…³ä¿¡æ¯ã€‚\n",
    "\n",
    "æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨Glove Word vectorï¼Œå…¶é‡‡ç”¨[Glove](https://nlp.stanford.edu/projects/glove/)ç®—æ³•åœ¨å¤§é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ä¸º50ï¼Œä¸‹é¢æˆ‘ä»¬åŠ è½½embedding matrixåŠå…¶å¯¹åº”çš„è½¬æ¢æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from os.path import isfile\n",
    "\n",
    "file_name = 'glove.6B.50d.txt'\n",
    "zip_file_name = 'glove.6B.50d.zip'\n",
    "\n",
    "##è§£å‹ç¼©\n",
    "if not isfile(file_name):\n",
    "    zip_file = zipfile.ZipFile(zip_file_name)\n",
    "\n",
    "    for names in zip_file.namelist():\n",
    "        zip_file.extract(names,'./')\n",
    "\n",
    "    zip_file.close()\n",
    "\n",
    "## è¯»å–æ–‡ä»¶\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢å‡½æ•°è¿”å›ï¼š\n",
    "\n",
    "- `word_to_index`:å°†Wordæ˜ å°„åˆ°indexçš„å­—å…¸(400,001 ä¸ªå•è¯, indexçš„æœ‰æ•ˆèŒƒå›´æ˜¯ 0 to 400,000)\n",
    "- `index_to_word`: å°†indexæ˜ å°„ä¸ºå¯¹åº”è¯å…¸ä¸­ç›¸åº”çš„Wordçš„å­—å…¸\n",
    "- `word_to_vec_map`: å­—å…¸ï¼Œå°†Wordæ˜ å°„ä¸ºå¯¹åº”çš„GloVe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of riki in the vocabulary is 308731\n",
      "the 289846th word in the vocabulary is potatos\n",
      "[ 0.028124 -0.63486  -0.27306   0.59     -0.70151  -0.6989   -0.022798\n",
      " -0.036259  0.13135  -0.37172   0.19088   0.6423   -0.18758  -0.032136\n",
      "  0.41346  -0.29416   0.49238  -0.32997   0.31169   1.3649   -0.80761\n",
      "  0.59873  -0.37682   0.71334   1.4189    0.86403   2.1224   -0.4706\n",
      " -0.51496   0.5896   -0.43853   0.5058    0.77305   0.18307  -1.4219\n",
      "  1.4845    0.085943 -0.18945   0.29573   0.27643   0.76693   0.94809\n",
      " -0.37852  -0.45716   0.36598   0.61302   0.55279  -0.38215   0.28966\n",
      "  0.050976]\n"
     ]
    }
   ],
   "source": [
    "# å¯ä»¥å°è¯•ä¸€ä¸‹ï¼Œè¿™ä¸ªè¯å…¸è¿˜æ˜¯æŒºå…¨çš„\n",
    "word = \"riki\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print (word_to_vec_map[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥è¦å®Œæˆä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°è´Ÿè´£å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¨¡å‹æœ‰æ•ˆçš„è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯å°†Wordæ˜ å°„ä¸ºGloveçš„è¡¨ç¤ºæ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    # è¯å‘é‡é‡Œé¢éƒ½æ˜¯å°å†™ï¼Œæ‰€ä»¥è¿™é‡Œè½¬æ¢ä¸ºå°å†™\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    avg = np.zeros(50)\n",
    "    \n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg/len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model\n",
    "\n",
    "æœ‰äº†è¾“å…¥ä¹‹åå°±å¯ä»¥æ‰§è¡Œforward propagationå’Œbackward propagationå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒäº†ï¼Œmodelå‡½æ•°å®Œæˆä¸‹é¢çš„åŠŸèƒ½ï¼Œç”±äºåªæœ‰ä¸€ä¸ªsoftmaxæ¯”è¾ƒç®€å•ï¼Œè¿™é‡Œæˆ‘å†³å®šæ‰‹åŠ¨æ¥æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          # æ ·æœ¬æ•°é‡\n",
    "    n_y = 5                                 # ç±»åˆ«æ•°é‡\n",
    "    n_h = 50                                # è¾“å…¥ç»´åº¦ï¼Œä¹Ÿæ˜¯Gloveçš„ç»´åº¦\n",
    "    \n",
    "    # ä½¿ç”¨Xavieræ¥åˆå§‹åŒ–W\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # å¾ªç¯\n",
    "    for t in range(num_iterations):                       \n",
    "        for i in range(m):\n",
    "            \n",
    "            # è·å–è¾“å…¥å¥å­çš„å¹³å‡Glove\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            z = np.dot(W, avg.reshape(-1, 1)) + b.reshape(-1, 1)\n",
    "            a = softmax(z.reshape(-1))\n",
    "\n",
    "            # æŸå¤±å‡½æ•°ï¼šcross entropy\n",
    "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # æ›´æ–°æ¢¯åº¦\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "(132,)\n",
      "(132, 5)\n",
      "never talk to me again\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(132, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810076\n",
      "Accuracy: 0.3484848484848485\n",
      "Epoch: 100 --- cost = 0.07971818726014794\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 200 --- cost = 0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "Epoch: 300 --- cost = 0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n",
      "[[3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶å®æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿ä½¿ç”¨å¦‚ä½•ç®€å•çš„æ¨¡å‹ï¼Œå¦‚æ­¤å°‘é‡çš„æ ·æœ¬ï¼Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„åˆ†æ•°è¿˜æ˜¯å¾ˆé«˜çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.4 - ä¸‹é¢æ¥çœ‹çœ‹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šé¢çš„è¡¨ç°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.977272727273\n",
      "Test set:\n",
      "Accuracy: 0.857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬çš„æ¨¡å‹æœ‰5ä¸ªåˆ†ç±»ï¼Œå¦‚æœæ˜¯ä¹±çŒœçš„è¯æ­£ç¡®ç‡ä¼šæœ‰20%ï¼Œåœ¨å¦‚æ­¤å°‘é‡çš„è®­ç»ƒæ ·æœ¬ä¸Šè¿™ä¸ªè¡¨ç°æ˜¯ç›¸å½“ä¸é”™çš„ã€‚åœ¨è®­ç»ƒé›†é‡Œé¢ï¼Œ\"*I love you*\" è¿™ä¸ªæ ·æœ¬çš„labelæ˜¯ â¤ï¸ï¼Œè€Œ\"adore\"è¿™ä¸ªå•è¯æ˜¯æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒé›†ä¸­çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ¥çœ‹çœ‹è¾“å…¥\"*I adore you*.\"ï¼Œæ¨¡å‹ä¾ç„¶å¯ä»¥æ­£ç¡®åˆ¤æ–­å—ï¼Ÿï¼ˆéªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "\n",
      "it is good ğŸ˜„\n",
      "i adore you â¤ï¸\n",
      "funny lol ğŸ˜„\n",
      "lets play with a ball âš¾\n",
      "food is ready ğŸ´\n",
      "not feeling happy ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"it is good\", \"i adore you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å“ˆå“ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç®—æ³•æˆåŠŸåœ°å°†\"I adore you.\"é¢„æµ‹æ­£ç¡®ï¼Œå› ä¸ºåœ¨Gloveçš„è¡¨ç¤ºæ–¹æ³•ä¸­loveå’Œadoreæ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ‰€ä»¥ç®—æ³•å°†loveæ³›åŒ–åˆ°äº†adoreã€‚\n",
    "\n",
    "ä½†æ˜¯è¯·æ³¨æ„ï¼Œæœ€åä¸€ä¸ªæµ‹è¯•è¯­â€œnot feeling happyâ€è¢«è¯¯åˆ†ç±»ä¸ºhappyçš„ç¬‘è„¸äº†ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¿™ä¸ªç®€å•æ¨¡å‹çš„ç¼ºé™·äº†ï¼Œå› ä¸ºè¯¥æ¨¡å‹ä»…ä»…æ•æ‰äº†æ•´å¥è¯çš„å¹³å‡ä¿¡æ¯è€Œå¿½ç•¥çš„è¯è¯­çš„å‰åé¡ºåºï¼Œæ‰€ä»¥å…¶æ— æ³•ç†è§£ç±»ä¼¼â€œnot happyâ€è¿™ç§å¥å­ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å•ç®—æ³•çš„æ€»ç»“\n",
    "\n",
    "1ï¼Œä½¿ç”¨äº†å°‘é‡çš„æ ·æœ¬è®­ç»ƒå‡ºäº†ä¸é”™çš„æ•ˆæœï¼Œè¿™éƒ½å½’åŠŸäºgloveçš„è¯å‘é‡è¡¨è¾¾æ–¹å¼\n",
    "\n",
    "2ï¼Œä½†æ˜¯è¿™ç§ç®€ç­”çš„æ¨¡å‹åœ¨å…³ä¹é¡ºåºçš„å¥å­ä¸­æ˜¯è¡¨ç°å¾ˆå·®çš„ï¼Œå› ä¸ºå®ƒä»…ä»…å°†å„ä¸ªè¯è¯­å¹³å‡ï¼Œæå–å¹³å‡ä¿¡æ¯ï¼Œæ— æ³•æå–å¥å­é—´çš„é¡ºåºä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - RNN with LSTMï¼ˆkeraså®ç°ï¼‰\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬åˆ©ç”¨RNNï¼ˆå…¶ä¸­time stepé‡‡ç”¨LSTMæ¥å®ç°ï¼‰æ¥æ„å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥ä¼°è®¡åˆ°å•è¯é—´çš„é¡ºåºä¿¡æ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†ç»§ç»­é‡‡ç”¨Gloveä½œä¸ºè¯å‘é‡çš„è¡¨ç°æ–¹å¼ã€‚æˆ‘ä»¬å°†é‡‡ç”¨kerasæ¥å¿«é€Ÿæ­å»ºRNNæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riki/anaconda3/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - æ¨¡å‹\n",
    "\n",
    "ä¸‹é¢å°±æ˜¯æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šå¯¹æ¨¡å‹è¿›è¡Œç®€å•åœ°è§£é‡Š\n",
    "\n",
    "<img src=\"emojifier-v2.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 mini-batchæ³¨æ„äº‹é¡¹ï¼ˆkerasä¸­çš„RNNï¼‰\n",
    "\n",
    "æˆ‘ä»¬å¸Œæœ›ä»¥mini-batchçš„å½¢å¼æ¥è®­ç»ƒRNNï¼Œè¿™æ ·æ›´é«˜æ•ˆã€‚ç„¶è€Œï¼Œç»å¤§éƒ¨åˆ†çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½è¦æ±‚åŒä¸€mini-batchçš„æ‰€æœ‰çš„åºåˆ—éƒ½æ‹¥æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™æ ·çš„è¯æ‰èƒ½å®ç°å‘é‡åŒ–è¿ç®—ã€‚å› ä¸ºå¦‚æœmini-batchä¸­å¤šä¸ªæ ·æœ¬çš„å¥å­é•¿åº¦ä¸ä¸€è‡´çš„è¯ï¼Œå°†ä¼šå¯¼è‡´ä¸èƒ½åŒæ—¶è¿ç®—ã€‚\n",
    "\n",
    "è¿™ç§é—®é¢˜çš„ä¸€èˆ¬è§£å†³åŠæ³•å°±æ˜¯paddingï¼Œæœ‰ç‚¹å„¿åƒCNNä¸­çš„é‚£ç§paddingï¼Œé¦–å…ˆæŒ‡å®šä¸€ä¸ªæœ€é•¿æ ·æœ¬é•¿åº¦ï¼Œç„¶åå°†æ‰€æœ‰é•¿åº¦ä¸è¶³æœ€é•¿é•¿åº¦çš„æ ·æœ¬éƒ½paddingåˆ°æœ€é•¿é•¿åº¦ï¼Œå…·ä½“çš„paddingæ–¹æ³•å°±æ˜¯åŠ å…¥0è¯å‘é‡ã€‚å¦‚æœæ ·æœ¬çš„é•¿åº¦è¶…è¿‡æœ€é•¿é•¿åº¦é‚£å°±å»æ‰è¶…å‡ºéƒ¨åˆ†ã€‚ä¸€ä¸ªé€‰æ‹©æœ€é•¿é•¿åº¦çš„æ–¹æ³•å°±æ˜¯å–è®­ç»ƒé›†ä¸­é•¿åº¦æœ€é•¿çš„å¥å­çš„é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - The Embedding layer\n",
    "\n",
    "åœ¨kerasä¸­ï¼Œembedding matrixè¡¨ç¤ºä¸ºä¸€ä¸ªâ€œlayerâ€ï¼Œå®ƒå°†è¾“å…¥çš„æ­£æ•´æ•°ï¼ˆå¯¹åº”Wordçš„indexï¼‰æ˜ å°„ä¸ºå›ºå®šå¤§å°çš„å‘é‡(è¯å‘é‡)ã€‚embedding layerå¯ä»¥è¢«ä¸€ä¸ªé¢„è®­ç»ƒçš„embedding vectoråˆå§‹åŒ–ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Glove-50æ¥åˆå§‹åŒ–è¿™ä¸ªlayerï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸æ‰“ç®—è¿›ä¸€æ­¥æ›´æ–°/è®­ç»ƒè¿™ä¸ªlayerï¼Œå› ä¸ºæˆ‘ä»¬çš„è®­ç»ƒé›†å¤ªå°äº†ï¼Œæ›´æ–°embedding vectoråè€Œä¼šé€ æˆæ›´å·®çš„æ•ˆæœã€‚kerasçš„embedding layeræä¾›äº†æ§åˆ¶æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°embedding vectorçš„é€‰é¡¹\n",
    "\n",
    "<img src=\"embedding1.png\" style=\"width:700px;height:250px;\">\n",
    "\n",
    "å…¶ä¸­embedding layerè¾“å…¥çš„æœ€å¤§çš„æ•´æ•°ï¼ˆWord indexï¼‰åº”è¯¥ä¸å¤§äºå­—å…¸çš„å¤§å°ï¼Œembedding layerçš„è¾“å‡ºç»´åº¦ä¸º (batch size, max input length, dimension of word vectors).\n",
    "\n",
    "æ‰€ä»¥ç¬¬ä¸€æ­¥åº”è¯¥æ˜¯å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºindexçš„æ•°ç»„ï¼Œç„¶åå¯¹æ ·æœ¬è¿›è¡Œzero-paddingä»¥å¯¹é½æœ€å¤§é•¿åº¦ã€‚\n",
    "\n",
    "ä¸‹é¢çš„å‡½æ•°å®ç°äº†å°†è¾“å…¥çš„batch samplesè½¬æ¢ä¸ºbatch indexã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "        å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºindexçš„å½¢å¼\n",
    "    \n",
    "        å‚æ•°ï¼š\n",
    "            Xï¼šè¾“å…¥çš„å¥å­çš„æ•°ç»„ï¼Œå¤§å°ä¸º(m,1)\n",
    "            word_to_indexï¼šWordåˆ°indexçš„æ˜ å°„\n",
    "            max_len:å¯¹é½çš„æœ€å¤§é•¿åº¦\n",
    "        è¿”å›ï¼š\n",
    "            indexå½¢å¼çš„æ ·æœ¬ï¼Œå¤§å°ä¸º(m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    X_indices = np.zeros((len(X), max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    vocab_len = len(word_to_index)+1                  # è¿™æ˜¯kerasè¦æ±‚çš„+1\n",
    "    #print (vocab_len)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # Word embeddingçš„é•¿åº¦\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 æ­å»ºæ¨¡å‹\n",
    "\n",
    "ä¸‹å›¾ä¸­çš„æ¨¡å‹å°±æ˜¯æˆ‘ä»¬è¦æ­å»ºçš„RNNæ¨¡å‹ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬ç›´æ¥å°†embedding layerè¾“å‡ºä½œä¸ºLSTMç½‘ç»œçš„è¾“å…¥ï¼Œå¹¶ä¸”åªåœ¨æœ€åä¸€ä¸ªtime stepè¾“å‡ºï¼Œè¾“å‡ºåŒæ ·ä¸ºsoftmaxï¼Œè¾“å‡ºçš„ç»´åº¦ä¸º(m, 5)ã€‚å¹¶ä¸”ä¸ºäº†æ›´å¤šçš„æ¨¡å‹å¤æ‚åº¦ï¼Œå¦‚ä¸‹çš„ç½‘ç»œé‡‡ç”¨deep RNNè¿›è¡Œäº†å †å ï¼Œå¹¶ä¸”åœ¨å„å±‚ä¹‹é—´åŠ å…¥äº†dropoutæ¥å‡å°è¿‡æ‹Ÿåˆçš„é£é™©ã€‚\n",
    "\n",
    "<img src=\"emojifier-v2.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    # å®šä¹‰æ¨¡å‹çš„è¾“å…¥å±‚ï¼Œè¯¥è¾“å…¥çš„å¤§å°ä¸º(m, max_len)ï¼Œè¾“å…¥çš„å†…å®¹æ˜¯ç»è¿‡äº†indexè½¬æ¢çš„æ ·æœ¬æ•°æ®\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # å°†embedding layeré“¾å¦‚æ¨¡å‹\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # æ¥ä¸‹æ¥æ­å»ºç¬¬ä¸€å±‚LSTMï¼Œè¿™ä¸€å±‚çš„éšè—å•å…ƒå¤§å°ä¸º128ï¼Œè€Œä¸”æ³¨æ„return_sequenceså‚æ•°è¡¨æ˜ç°åœ¨æ˜¯è¦è¿”å›æ•´ä¸ªsequenceçš„è¾“å‡ºï¼ˆTrueï¼‰è¿˜æ˜¯åªè¿”å›æœ€åä¸€ä¸ª\n",
    "    # time stepçš„è¾“å‡ºï¼ˆFalseï¼‰ã€‚è¿™é‡Œæˆ‘ä»¬çš„ç¬¬ä¸€å±‚LSTMæ˜¯è¦è¿”å›æ•´ä¸ªåºåˆ—çš„è¾“å‡ºï¼Œæ‰€ä»¥ä¸ºTrueã€‚\n",
    "    X = LSTM(units=128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    # æ­å»ºç¬¬äºŒå±‚çš„LSTMï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬åªéœ€è¦è¿”å›æœ€åä¸€ä¸ªtimestepçš„è¾“å‡º\n",
    "    X = LSTM(units=128)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    #X = softmax(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®°ä¸‹æ¥æˆ‘ä»¬è¿è¡Œä¸Šé¢çš„å‡½æ•°æ¥æ­å»ºæ¨¡å‹ã€‚ç”±äºæ•°æ®é›†ä¸­çš„æ ·æœ¬é•¿åº¦å‡å°äº10ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„max_lenå°±é€‰æ‹©10.\n",
    "\n",
    "ç„¶åæˆ‘ä»¬æ‰“å°æ¨¡å‹çš„summaryæ¥çœ‹çœ‹ï¼Œæ€»å…±å‚æ•°ä¸º20223927ï¼Œä½†æ˜¯å¯è®­ç»ƒçš„å‚æ•°ä»…ä»…ä¸º223887ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„embedding layerå°±å ç”¨äº†20000050ä¸ªå‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¦compileæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå› ä¸ºè¾“å‡ºä¸ºsoftmaxæ‰€ä»¥lossé€‰æ‹©categorical_crossentropyã€‚ç„¶åoptimizeré‡‡ç”¨æ¨¡å‹çš„Adamé…ç½®ï¼Œè¡¡é‡æ ‡å‡†é‡‡ç”¨accuracyï¼Œä»£è¡¨é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢è®­ç»ƒçš„æ—¶å€™æˆ‘å¤§æ¦‚ç”¨äº†94ä¸ªepoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 1.5884 - acc: 0.2803\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 713us/step - loss: 1.5250 - acc: 0.2955\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 759us/step - loss: 1.4938 - acc: 0.3333\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4549 - acc: 0.3333\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3635 - acc: 0.3561\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2468 - acc: 0.4470\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 913us/step - loss: 1.1242 - acc: 0.5606\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 861us/step - loss: 1.0031 - acc: 0.6212\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 755us/step - loss: 0.9289 - acc: 0.6515\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 773us/step - loss: 0.8107 - acc: 0.7121\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 796us/step - loss: 0.7050 - acc: 0.7576\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 805us/step - loss: 0.7044 - acc: 0.7197\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 778us/step - loss: 0.5743 - acc: 0.7803\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 751us/step - loss: 0.6373 - acc: 0.8106\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 809us/step - loss: 0.5212 - acc: 0.8333\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 944us/step - loss: 0.3922 - acc: 0.8864\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 865us/step - loss: 0.2963 - acc: 0.9318\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 751us/step - loss: 0.3606 - acc: 0.8864\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 749us/step - loss: 0.2566 - acc: 0.8864\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 796us/step - loss: 0.2273 - acc: 0.9470\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 832us/step - loss: 0.2383 - acc: 0.9242\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 833us/step - loss: 0.3818 - acc: 0.8561\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 946us/step - loss: 0.2569 - acc: 0.9242\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 874us/step - loss: 0.1739 - acc: 0.9394\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 780us/step - loss: 0.1507 - acc: 0.9621\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 831us/step - loss: 0.1411 - acc: 0.9697\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 854us/step - loss: 0.1459 - acc: 0.9545\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 805us/step - loss: 0.1221 - acc: 0.9545\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 843us/step - loss: 0.0743 - acc: 0.9848\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 810us/step - loss: 0.0668 - acc: 0.9773\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 816us/step - loss: 0.0787 - acc: 0.9697\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 785us/step - loss: 0.0627 - acc: 0.9773\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 832us/step - loss: 0.0682 - acc: 0.9773\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 870us/step - loss: 0.0779 - acc: 0.9697\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 793us/step - loss: 0.1150 - acc: 0.9697\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 808us/step - loss: 0.0624 - acc: 0.9848\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 773us/step - loss: 0.0370 - acc: 0.9848\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 789us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 855us/step - loss: 0.0293 - acc: 0.9848\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0271 - acc: 0.9924\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 789us/step - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 836us/step - loss: 0.0142 - acc: 0.9924\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 830us/step - loss: 0.0200 - acc: 0.9924\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 822us/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 853us/step - loss: 0.0156 - acc: 0.9924\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 822us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 747us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 742us/step - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 801us/step - loss: 0.0166 - acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1adbab86a0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n",
    "##validation_split=0.2, \n",
    "##initial_epoch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºæœ¬é¡¹ç›®çš„æ ·æœ¬æ˜¯åœ¨å¤ªå°‘ï¼Œæ‰€ä»¥æˆ‘åœ¨fitçš„æ—¶å€™å¹¶æ²¡æœ‰åˆ’åˆ†éªŒè¯é›†ç”¨äºæŸ¥çœ‹éªŒè¯é›†çš„å‡†ç¡®ç‡ã€‚æ€»å…±æ‰100å¤šä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†åˆ†é…å¤šäº†ï¼Œæ¨¡å‹è‚¯å®šè¿‡æ‹Ÿåˆï¼ŒéªŒè¯é›†åˆ†é…å°‘äº†ï¼Œé‚£éªŒè¯çš„ç»“æœåˆä¸å…·æœ‰ä»£è¡¨æ€§ï¼Œæ‰€ä»¥è¿™é‡Œå°±ä¸çº ç»“äºè°ƒå‚çš„è¿‡ç¨‹äº†ã€‚è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦ç›®çš„è¿˜æ˜¯ä¸ºäº†å­¦ä¹ RNNè¿˜æœ‰Word embeddingç­‰æ¦‚å¿µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 12ms/step\n",
      "\n",
      "Test accuracy =  0.8392857142857143\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:ğŸ˜„ prediction: she got me a nice present\tâ¤ï¸\n",
      "Expected emoji:ğŸ˜ prediction: work is hard\tğŸ˜„\n",
      "Expected emoji:ğŸ˜ prediction: This girl is messing with me\tâ¤ï¸\n",
      "Expected emoji:ğŸ˜ prediction: work is horrible\tğŸ˜„\n",
      "Expected emoji:ğŸ˜„ prediction: you brighten my day\tâ¤ï¸\n",
      "Expected emoji:ğŸ˜ prediction: she is a bully\tğŸ˜„\n",
      "Expected emoji:ğŸ˜ prediction: My life is so boring\tâ¤ï¸\n",
      "Expected emoji:ğŸ˜ prediction: I do not want to joke\tâ¤ï¸\n",
      "Expected emoji:ğŸ˜ prediction: go away\tâš¾\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹é¢æŸ¥çœ‹åˆ’åˆ†é”™è¯¯çš„å¥å­\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆæ­¥åŸå› åˆ†æè¿˜æ˜¯è®­ç»ƒé›†çš„æ ·æœ¬å¤ªå°‘ï¼Œè™½ç„¶Word embeddingæ­å»ºçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¥½ï¼Œä½†æ˜¯å¦‚æœç±»ä¼¼çš„æ ·æœ¬ä¸€æ¬¡éƒ½æ²¡è§è¿‡çš„è¯ï¼Œé‚£ä¹ˆä¹Ÿæ˜¯æ— æ³•è¿›è¡Œæ³›åŒ–çš„ã€‚\n",
    "\n",
    "ä¸‹é¢å°è¯•ä¸€ä¸‹è‡ªå·±æƒ³è¾“å…¥çš„å¥å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹‹å‰æˆ‘ä»¬ç®€å•çš„æ¨¡å‹æ— æ³•åˆ†è¾¨å‡ºnot feeling happy,ä½†æ˜¯ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥äº†ã€‚ä½†æ˜¯å½“å‰æ¨¡å‹ä¸èƒ½åˆ†è¾¨not happyï¼Œæ˜¯å› ä¸ºè´Ÿé¢æƒ…ç»ªçš„æ ·æœ¬å¤ªå°‘äº†ã€‚å¦‚æœè®­ç»ƒé›†æ›´åºå¤§çš„è¯é‚£ä¹ˆè¯¥æ¨¡å‹è‚¯å®šä¼šè¡¨ç°æ›´å‡ºè‰²ã€‚"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

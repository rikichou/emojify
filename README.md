
## æƒ…æ„Ÿåˆ†æ

æˆ‘ä»¬éƒ½æœ‰è¿‡ç½‘ä¸Šè´­ç‰©çš„ç»éªŒï¼Œä¸€èˆ¬åœ¨ç¡®è®¤æ”¶è´§çš„æ—¶å€™ç½‘ç«™éƒ½ä¼šè®©æˆ‘ä»¬ä¸ºå•†å“ç•™ä¸‹è¯„è®ºå¹¶ä¸”æ‰“æ˜Ÿæ˜Ÿï¼Œä¸€èˆ¬æ˜¯1æ˜Ÿåˆ°5æ˜Ÿï¼Œæ˜Ÿçº§çš„å¤šå°‘ä¹Ÿä»£è¡¨äº†æˆ‘ä»¬è¿™æ¬¡è´­ç‰©çš„æ»¡æ„ç¨‹åº¦ã€‚é‚£æˆ‘ä»¬èƒ½ä¸èƒ½æ ¹æ®è¯„è®ºçš„å†…å®¹æ¥æ¨æ–­å®¢æˆ·æœ¬æ¬¡è´­ç‰©çš„æ»¡æ„ç¨‹åº¦å‘¢ï¼Ÿäººå½“ç„¶æ˜¯å¯ä»¥çš„ï¼Œä½†æ˜¯ä»Šå¤©æˆ‘ä»¬è¦è®©ç®—æ³•æ¥åšç±»ä¼¼çš„äº‹æƒ…ã€‚

## Emojify

ç°åœ¨ï¼Œæœ‰è®¸å¤šè„šæœ¬å¯ä»¥åœ¨æˆ‘ä»¬çš„å†…å®¹ä¸­è‡ªåŠ¨æ’å…¥ä»£è¡¨è¯­ä¹‰çš„è¡¨æƒ…ç¬¦å·ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¥è‹±æ–‡ï¼Œ"Congratulations on the promotion! Lets get coffee and talk. Love you!" è¾“å…¥ç›¸åº”çš„è„šæœ¬å°±å¯ä»¥å°†å…¶è½¬æ¢ä¸ºï¼š "Congratulations on the promotion! ğŸ‘ Lets get coffee and talk. â˜•ï¸ Love you! â¤ï¸"ã€‚ä½†æ˜¯è¿™äº›è„šæœ¬ä»…ä»…æ˜¯æ ¹æ®å…³é”®å­—çš„è®°å¿†ä»¥åŠåŒ¹é…æ¥å®Œæˆçš„ï¼Œæ— æ³•çœŸæ­£åšåˆ°è¯­ä¹‰çš„ç†è§£ï¼Œè€Œä¸”æ³›åŒ–èƒ½åŠ›ä¸å¼ºï¼Œå¦‚æœæŸä¸ªå…³é”®å­—ä¸å¤„äºè„šæœ¬å†…çš„å…³é”®å­—åˆ—è¡¨å†…çš„è¯å°±æ— æ³•åŒ¹é…ã€‚æ‰€ä»¥æœ¬é¡¹ç›®å°†ä½¿ç”¨RNN+LSTM+Word embeddingæ¥å®Œæˆï¼Œå½“ç„¶ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬è¿˜ä¼šä½¿ç”¨ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆæ¥ä½œä¸ºå¯¹ç…§ã€‚


```python
import numpy as np
from emo_utils import *
import emoji
import matplotlib.pyplot as plt

%matplotlib inline
```

## 1 -ç®€å•çš„æ–¹æ¡ˆ--softmax regression

### 1.1 - æå–æ•°æ®é›†

æˆ‘ä»¬çš„æ•°æ®é›†ç”±127ä¸ªè®­ç»ƒæ ·æœ¬å’Œ56ä¸ªæµ‹è¯•æ ·æœ¬ç»„æˆï¼ˆæ ·æœ¬æœ‰ç‚¹å„¿å°‘å“ˆï¼Œä½†æ˜¯è¿™æ ·æ‰èƒ½ä½“ç°å‡ºWord embeddingçš„å¨åŠ›ï¼‰ã€‚

å…¶ä¸­Yæ˜¯èŒƒå›´ä¸º0--4çš„æ•´æ•°ï¼Œä»£è¡¨äº†æ¯ä¸ªå¥å­å¯¹åº”çš„è¡¨æƒ…ç¬¦å·ã€‚

ä¸‹å›¾ç»™äº†éƒ¨åˆ†çš„æ ·æœ¬ï¼Œé€šè¿‡è§‚å¯Ÿå¯ä»¥å¤§è‡´äº†è§£æ ·æœ¬çš„æƒ…å†µã€‚

<img src="data_set.png" style="width:700px;height:300px;">


```python
X_train, Y_train = read_csv('train_emoji.csv')
X_test, Y_test = read_csv('test.csv')
```


```python
maxLen = len(max(X_train, key=len).split())
```

ä¸‹é¢åšä¸€ä¸‹ç®€å•çš„æ•°æ®å¯è§†åŒ–ï¼Œindexä»£è¡¨æƒ³è¦æŸ¥çœ‹çš„æ ·æœ¬çš„indexï¼Œä¸‹é¢çš„å¥å­å°†ä¼šæ‰“å°å‡ºæ ·æœ¬å¥å­ä¸å…¶å¯¹åº”çš„è¡¨æƒ…ç¬¦å·ï¼ˆå…¶ä¸­è¡¨æƒ…ç¬¦å·ç”¨åˆ°äº†emojiåº“ï¼Œè¯¥åº“æä¾›äº†ä¸°å¯Œçš„è¡¨æƒ…ç¬¦å·ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è‡ªå·±è¯•è¯•[emoji](https://pypi.org/project/emoji/)


```python
index = 9
print(X_train[index], label_to_emoji(Y_train[index]))
```

    I want to go play âš¾


### 1.2 - æ¨¡å‹

ä¸‹é¢æˆ‘ä»¬å°±æ¥å®ç°ä¸‹å›¾ä¸­çš„è¿™ä¸ªæ¨¡å‹

<img src="image_1.png" style="width:900px;height:300px;">

è¯¥æ¨¡å‹çš„è¾“å…¥ä¸ºå¥å­çš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬è§£æå‡ºæ¯ä¸ªå­—ç¬¦ä¸²çš„å•è¯ï¼Œç„¶åå°†å…¶ç”¨è¯å‘é‡è¡¨ç¤ºï¼ˆæ¯”å¦‚50ç»´åº¦çš„è¯å‘é‡ï¼‰ï¼Œç„¶åå°†æ‰€æœ‰çš„è¯å‘é‡å–å¹³å‡ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œè¿™æ ·ï¼Œä¸ç®¡å¥å­çš„é•¿çŸ­ï¼Œéƒ½å¯ä»¥æ­£ç¡®è¾“å…¥æ¨¡å‹ã€‚ç„¶åé€šè¿‡ä¸€ä¸ª5ä¸ªéšè—å•å…ƒçš„softmaxæ¥è¿›è¡Œåˆ†ç±»è¾“å‡ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨argmaxæ¥åˆ¤æ–­å“ªä¸ªè¡¨æƒ…çš„å¯èƒ½æ€§æœ€å¤§

ä¸ºäº†åŒ¹é…è¾“å‡ºçš„ç»´åº¦ï¼Œæˆ‘ä»¬è¦å°†æ ·æœ¬ä¸­çš„æ ‡ç­¾ä»¥one-hot vectorçš„å½¢å¼è¡¨ç°å‡ºæ¥


```python
Y_oh_train = convert_to_one_hot(Y_train, C = 5)
Y_oh_test = convert_to_one_hot(Y_test, C = 5)
```


```python
index = 50
print(Y_train[index], "is converted into one hot", Y_oh_train[index])
```

    0 is converted into one hot [1. 0. 0. 0. 0.]


### 1.3 - æ¨¡å‹æ­å»ºä»¥åŠè®­ç»ƒ

ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºæ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†è¾“å…¥çš„å¥å­ä»¥è¯å‘é‡çš„å½¢å¼è¡¨ç¤ºï¼Œç„¶åå†å¯¹ä»–ä»¬è¿›è¡Œå¹³å‡ã€‚æ™®é€šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨vocabularyï¼ˆè¯æ±‡è¡¨ï¼‰ç„¶åå†åŠ ä¸Šone-hot representationæ¥è¡¨ç¤ºï¼Œè¿™ç§è¡¨ç¤ºæ–¹å¼çš„ä¸€å¤§ç¼ºç‚¹å°±æ˜¯ï¼Œå®ƒæŠŠæ¯ä¸ªè¯éƒ½å­¤ç«‹èµ·æ¥ï¼Œè¿™æ ·ä½¿å¾—ç®—æ³•å¯¹ç›¸å…³è¯çš„æ³›åŒ–èƒ½åŠ›ä¸å¼ºï¼Œæ¯”å¦‚æ¨¡å‹çŸ¥é“orange juiceï¼Œä½†æ˜¯å¾ˆæœ‰å¯èƒ½æ— æ³•å°†å…¶æ³›åŒ–ä¸ºapple juiceï¼ˆå¦‚æœè®­ç»ƒé›†ä¸­æ²¡æœ‰apple juiceçš„è¯ï¼‰ã€‚ç”±äºä¸¤ä¸ªè¯è¯­çš„one-hot vectorçš„ä¹˜ç§¯ä¸º0ï¼Œæ‰€ä»¥å…¶æœ¬èº«æ— æ³•è¡¨ç¤ºä¸¤ä¸ªè¯è¯­çš„ä»»ä½•å…³ç³»ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œé‡‡ç”¨é¢„è®­ç»ƒçš„Word embeddingï¼ŒWord embeddingæ˜¯ç»è¿‡åºå¤§çš„æ•°æ®é›†è®­ç»ƒï¼ˆ10äº¿ç”šè‡³100äº¿çº§åˆ«ï¼‰çš„è¯å‘é‡ï¼Œå…¶èƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨è¾¾å•è¯ä¹‹é—´çš„ç›¸å…³ä¿¡æ¯ã€‚

æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨Glove Word vectorï¼Œå…¶é‡‡ç”¨[Glove](https://nlp.stanford.edu/projects/glove/)ç®—æ³•åœ¨å¤§é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ä¸º50ï¼Œä¸‹é¢æˆ‘ä»¬åŠ è½½embedding matrixåŠå…¶å¯¹åº”çš„è½¬æ¢æ–¹æ³•ã€‚


```python
import zipfile
from os.path import isfile

file_name = 'glove.6B.50d.txt'
zip_file_name = 'glove.6B.50d.zip'

##è§£å‹ç¼©
if not isfile(file_name):
    zip_file = zipfile.ZipFile(zip_file_name)

    for names in zip_file.namelist():
        zip_file.extract(names,'./')

    zip_file.close()

## è¯»å–æ–‡ä»¶
word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(file_name)
```

ä¸Šé¢å‡½æ•°è¿”å›ï¼š

- `word_to_index`:å°†Wordæ˜ å°„åˆ°indexçš„å­—å…¸(400,001 ä¸ªå•è¯, indexçš„æœ‰æ•ˆèŒƒå›´æ˜¯ 0 to 400,000)
- `index_to_word`: å°†indexæ˜ å°„ä¸ºå¯¹åº”è¯å…¸ä¸­ç›¸åº”çš„Wordçš„å­—å…¸
- `word_to_vec_map`: å­—å…¸ï¼Œå°†Wordæ˜ å°„ä¸ºå¯¹åº”çš„GloVe vector


```python
# å¯ä»¥å°è¯•ä¸€ä¸‹ï¼Œè¿™ä¸ªè¯å…¸è¿˜æ˜¯æŒºå…¨çš„
word = "riki"
index = 289846
print("the index of", word, "in the vocabulary is", word_to_index[word])
print("the", str(index) + "th word in the vocabulary is", index_to_word[index])
print (word_to_vec_map[word])
```

    the index of riki in the vocabulary is 308731
    the 289846th word in the vocabulary is potatos
    [ 0.028124 -0.63486  -0.27306   0.59     -0.70151  -0.6989   -0.022798
     -0.036259  0.13135  -0.37172   0.19088   0.6423   -0.18758  -0.032136
      0.41346  -0.29416   0.49238  -0.32997   0.31169   1.3649   -0.80761
      0.59873  -0.37682   0.71334   1.4189    0.86403   2.1224   -0.4706
     -0.51496   0.5896   -0.43853   0.5058    0.77305   0.18307  -1.4219
      1.4845    0.085943 -0.18945   0.29573   0.27643   0.76693   0.94809
     -0.37852  -0.45716   0.36598   0.61302   0.55279  -0.38215   0.28966
      0.050976]


æ¥ä¸‹æ¥è¦å®Œæˆä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°è´Ÿè´£å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¨¡å‹æœ‰æ•ˆçš„è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯å°†Wordæ˜ å°„ä¸ºGloveçš„è¡¨ç¤ºæ–¹å¼


```python
def sentence_to_avg(sentence, word_to_vec_map):
    # è¯å‘é‡é‡Œé¢éƒ½æ˜¯å°å†™ï¼Œæ‰€ä»¥è¿™é‡Œè½¬æ¢ä¸ºå°å†™
    words = sentence.lower().split()

    avg = np.zeros(50)
    
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg/len(words)
    
    return avg
```


```python
avg = sentence_to_avg("Morrocan couscous is my favorite dish", word_to_vec_map)
print("avg = ", avg)
```

    avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983
     -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867
      0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767
      0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061
      0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265
      1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925
     -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333
     -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433
      0.1445417   0.09808667]


#### Model

æœ‰äº†è¾“å…¥ä¹‹åå°±å¯ä»¥æ‰§è¡Œforward propagationå’Œbackward propagationå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒäº†ï¼Œmodelå‡½æ•°å®Œæˆä¸‹é¢çš„åŠŸèƒ½ï¼Œç”±äºåªæœ‰ä¸€ä¸ªsoftmaxæ¯”è¾ƒç®€å•ï¼Œè¿™é‡Œæˆ‘å†³å®šæ‰‹åŠ¨æ¥æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­


```python
def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):
    np.random.seed(1)

    m = Y.shape[0]                          # æ ·æœ¬æ•°é‡
    n_y = 5                                 # ç±»åˆ«æ•°é‡
    n_h = 50                                # è¾“å…¥ç»´åº¦ï¼Œä¹Ÿæ˜¯Gloveçš„ç»´åº¦
    
    # ä½¿ç”¨Xavieræ¥åˆå§‹åŒ–W
    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
    b = np.zeros((n_y,))

    Y_oh = convert_to_one_hot(Y, C = n_y) 
    
    # å¾ªç¯
    for t in range(num_iterations):                       
        for i in range(m):
            
            # è·å–è¾“å…¥å¥å­çš„å¹³å‡Glove
            avg = sentence_to_avg(X[i], word_to_vec_map)

            # å‰å‘ä¼ æ’­
            z = np.dot(W, avg.reshape(-1, 1)) + b.reshape(-1, 1)
            a = softmax(z.reshape(-1))

            # æŸå¤±å‡½æ•°ï¼šcross entropy
            cost = -np.sum(Y_oh[i]*np.log(a))
            
            # åå‘ä¼ æ’­
            dz = a - Y_oh[i]
            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
            db = dz

            # æ›´æ–°æ¢¯åº¦
            W = W - learning_rate * dW
            b = b - learning_rate * db
        
        if t % 100 == 0:
            print("Epoch: " + str(t) + " --- cost = " + str(cost))
            pred = predict(X, Y, W, b, word_to_vec_map)

    return pred, W, b
```


```python
print(X_train.shape)
print(Y_train.shape)
print(np.eye(5)[Y_train.reshape(-1)].shape)
print(X_train[0])
print(type(X_train))
Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])
print(Y.shape)

X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',
 'Lets go party and drinks','Congrats on the new job','Congratulations',
 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',
 'You totally deserve this prize', 'Let us go play football',
 'Are you down for football this afternoon', 'Work hard play harder',
 'It is suprising how people can be dumb sometimes',
 'I am very disappointed','It is the best day in my life',
 'I think I will end up alone','My life is so boring','Good job',
 'Great so awesome'])

print(X.shape)
print(np.eye(5)[Y_train.reshape(-1)].shape)
print(type(X_train))

```

    (132,)
    (132,)
    (132, 5)
    never talk to me again
    <class 'numpy.ndarray'>
    (20,)
    (20,)
    (132, 5)
    <class 'numpy.ndarray'>



```python
pred, W, b = model(X_train, Y_train, word_to_vec_map)
print(pred)
```

    Epoch: 0 --- cost = 1.9520498812810076
    Accuracy: 0.3484848484848485
    Epoch: 100 --- cost = 0.07971818726014794
    Accuracy: 0.9318181818181818
    Epoch: 200 --- cost = 0.04456369243681402
    Accuracy: 0.9545454545454546
    Epoch: 300 --- cost = 0.03432267378786059
    Accuracy: 0.9696969696969697
    [[3.]
     [2.]
     [3.]
     [0.]
     [4.]
     [0.]
     [3.]
     [2.]
     [3.]
     [1.]
     [3.]
     [3.]
     [1.]
     [3.]
     [2.]
     [3.]
     [2.]
     [3.]
     [1.]
     [2.]
     [3.]
     [0.]
     [2.]
     [2.]
     [2.]
     [1.]
     [4.]
     [3.]
     [3.]
     [4.]
     [0.]
     [3.]
     [4.]
     [2.]
     [0.]
     [3.]
     [2.]
     [2.]
     [3.]
     [4.]
     [2.]
     [2.]
     [0.]
     [2.]
     [3.]
     [0.]
     [3.]
     [2.]
     [4.]
     [3.]
     [0.]
     [3.]
     [3.]
     [3.]
     [4.]
     [2.]
     [1.]
     [1.]
     [1.]
     [2.]
     [3.]
     [1.]
     [0.]
     [0.]
     [0.]
     [3.]
     [4.]
     [4.]
     [2.]
     [2.]
     [1.]
     [2.]
     [0.]
     [3.]
     [2.]
     [2.]
     [0.]
     [3.]
     [3.]
     [1.]
     [2.]
     [1.]
     [2.]
     [2.]
     [4.]
     [3.]
     [3.]
     [2.]
     [4.]
     [0.]
     [0.]
     [3.]
     [3.]
     [3.]
     [3.]
     [2.]
     [0.]
     [1.]
     [2.]
     [3.]
     [0.]
     [2.]
     [2.]
     [2.]
     [3.]
     [2.]
     [2.]
     [2.]
     [4.]
     [1.]
     [1.]
     [3.]
     [3.]
     [4.]
     [1.]
     [2.]
     [1.]
     [1.]
     [3.]
     [1.]
     [0.]
     [4.]
     [0.]
     [3.]
     [3.]
     [4.]
     [4.]
     [1.]
     [4.]
     [3.]
     [0.]
     [2.]]


å…¶å®æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿ä½¿ç”¨å¦‚ä½•ç®€å•çš„æ¨¡å‹ï¼Œå¦‚æ­¤å°‘é‡çš„æ ·æœ¬ï¼Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„åˆ†æ•°è¿˜æ˜¯å¾ˆé«˜çš„

### 1.4 - ä¸‹é¢æ¥çœ‹çœ‹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šé¢çš„è¡¨ç°



```python
print("Training set:")
pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)
print('Test set:')
pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)
```

    Training set:
    Accuracy: 0.977272727273
    Test set:
    Accuracy: 0.857142857143


æˆ‘ä»¬çš„æ¨¡å‹æœ‰5ä¸ªåˆ†ç±»ï¼Œå¦‚æœæ˜¯ä¹±çŒœçš„è¯æ­£ç¡®ç‡ä¼šæœ‰20%ï¼Œåœ¨å¦‚æ­¤å°‘é‡çš„è®­ç»ƒæ ·æœ¬ä¸Šè¿™ä¸ªè¡¨ç°æ˜¯ç›¸å½“ä¸é”™çš„ã€‚åœ¨è®­ç»ƒé›†é‡Œé¢ï¼Œ"*I love you*" è¿™ä¸ªæ ·æœ¬çš„labelæ˜¯ â¤ï¸ï¼Œè€Œ"adore"è¿™ä¸ªå•è¯æ˜¯æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒé›†ä¸­çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ¥çœ‹çœ‹è¾“å…¥"*I adore you*."ï¼Œæ¨¡å‹ä¾ç„¶å¯ä»¥æ­£ç¡®åˆ¤æ–­å—ï¼Ÿï¼ˆéªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼‰


```python
X_my_sentences = np.array(["it is good", "i adore you", "funny lol", "lets play with a ball", "food is ready", "not feeling happy"])
Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])

pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)
print_predictions(X_my_sentences, pred)
```

    Accuracy: 0.6666666666666666
    
    it is good ğŸ˜„
    i adore you â¤ï¸
    funny lol ğŸ˜„
    lets play with a ball âš¾
    food is ready ğŸ´
    not feeling happy ğŸ˜„


å“ˆå“ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç®—æ³•æˆåŠŸåœ°å°†"I adore you."é¢„æµ‹æ­£ç¡®ï¼Œå› ä¸ºåœ¨Gloveçš„è¡¨ç¤ºæ–¹æ³•ä¸­loveå’Œadoreæ˜¯å¾ˆç›¸ä¼¼çš„ï¼Œæ‰€ä»¥ç®—æ³•å°†loveæ³›åŒ–åˆ°äº†adoreã€‚

ä½†æ˜¯è¯·æ³¨æ„ï¼Œæœ€åä¸€ä¸ªæµ‹è¯•è¯­â€œnot feeling happyâ€è¢«è¯¯åˆ†ç±»ä¸ºhappyçš„ç¬‘è„¸äº†ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¿™ä¸ªç®€å•æ¨¡å‹çš„ç¼ºé™·äº†ï¼Œå› ä¸ºè¯¥æ¨¡å‹ä»…ä»…æ•æ‰äº†æ•´å¥è¯çš„å¹³å‡ä¿¡æ¯è€Œå¿½ç•¥çš„è¯è¯­çš„å‰åé¡ºåºï¼Œæ‰€ä»¥å…¶æ— æ³•ç†è§£ç±»ä¼¼â€œnot happyâ€è¿™ç§å¥å­ã€‚

### å•ç®—æ³•çš„æ€»ç»“

1ï¼Œä½¿ç”¨äº†å°‘é‡çš„æ ·æœ¬è®­ç»ƒå‡ºäº†ä¸é”™çš„æ•ˆæœï¼Œè¿™éƒ½å½’åŠŸäºgloveçš„è¯å‘é‡è¡¨è¾¾æ–¹å¼

2ï¼Œä½†æ˜¯è¿™ç§ç®€ç­”çš„æ¨¡å‹åœ¨å…³ä¹é¡ºåºçš„å¥å­ä¸­æ˜¯è¡¨ç°å¾ˆå·®çš„ï¼Œå› ä¸ºå®ƒä»…ä»…å°†å„ä¸ªè¯è¯­å¹³å‡ï¼Œæå–å¹³å‡ä¿¡æ¯ï¼Œæ— æ³•æå–å¥å­é—´çš„é¡ºåºä¿¡æ¯ã€‚

## 2 - RNN with LSTMï¼ˆkeraså®ç°ï¼‰

æ¥ä¸‹æ¥æˆ‘ä»¬åˆ©ç”¨RNNï¼ˆå…¶ä¸­time stepé‡‡ç”¨LSTMæ¥å®ç°ï¼‰æ¥æ„å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥ä¼°è®¡åˆ°å•è¯é—´çš„é¡ºåºä¿¡æ¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†ç»§ç»­é‡‡ç”¨Gloveä½œä¸ºè¯å‘é‡çš„è¡¨ç°æ–¹å¼ã€‚æˆ‘ä»¬å°†é‡‡ç”¨kerasæ¥å¿«é€Ÿæ­å»ºRNNæ¨¡å‹ã€‚


```python
import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)
```

    /home/riki/anaconda3/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters
    Using TensorFlow backend.


### 2.1 - æ¨¡å‹

ä¸‹é¢å°±æ˜¯æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šå¯¹æ¨¡å‹è¿›è¡Œç®€å•åœ°è§£é‡Š

<img src="emojifier-v2.png" style="width:700px;height:400px;">

### 2.2 mini-batchæ³¨æ„äº‹é¡¹ï¼ˆkerasä¸­çš„RNNï¼‰

æˆ‘ä»¬å¸Œæœ›ä»¥mini-batchçš„å½¢å¼æ¥è®­ç»ƒRNNï¼Œè¿™æ ·æ›´é«˜æ•ˆã€‚ç„¶è€Œï¼Œç»å¤§éƒ¨åˆ†çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½è¦æ±‚åŒä¸€mini-batchçš„æ‰€æœ‰çš„åºåˆ—éƒ½æ‹¥æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™æ ·çš„è¯æ‰èƒ½å®ç°å‘é‡åŒ–è¿ç®—ã€‚å› ä¸ºå¦‚æœmini-batchä¸­å¤šä¸ªæ ·æœ¬çš„å¥å­é•¿åº¦ä¸ä¸€è‡´çš„è¯ï¼Œå°†ä¼šå¯¼è‡´ä¸èƒ½åŒæ—¶è¿ç®—ã€‚

è¿™ç§é—®é¢˜çš„ä¸€èˆ¬è§£å†³åŠæ³•å°±æ˜¯paddingï¼Œæœ‰ç‚¹å„¿åƒCNNä¸­çš„é‚£ç§paddingï¼Œé¦–å…ˆæŒ‡å®šä¸€ä¸ªæœ€é•¿æ ·æœ¬é•¿åº¦ï¼Œç„¶åå°†æ‰€æœ‰é•¿åº¦ä¸è¶³æœ€é•¿é•¿åº¦çš„æ ·æœ¬éƒ½paddingåˆ°æœ€é•¿é•¿åº¦ï¼Œå…·ä½“çš„paddingæ–¹æ³•å°±æ˜¯åŠ å…¥0è¯å‘é‡ã€‚å¦‚æœæ ·æœ¬çš„é•¿åº¦è¶…è¿‡æœ€é•¿é•¿åº¦é‚£å°±å»æ‰è¶…å‡ºéƒ¨åˆ†ã€‚ä¸€ä¸ªé€‰æ‹©æœ€é•¿é•¿åº¦çš„æ–¹æ³•å°±æ˜¯å–è®­ç»ƒé›†ä¸­é•¿åº¦æœ€é•¿çš„å¥å­çš„é•¿åº¦ã€‚

### 2.3 - The Embedding layer

åœ¨kerasä¸­ï¼Œembedding matrixè¡¨ç¤ºä¸ºä¸€ä¸ªâ€œlayerâ€ï¼Œå®ƒå°†è¾“å…¥çš„æ­£æ•´æ•°ï¼ˆå¯¹åº”Wordçš„indexï¼‰æ˜ å°„ä¸ºå›ºå®šå¤§å°çš„å‘é‡(è¯å‘é‡)ã€‚embedding layerå¯ä»¥è¢«ä¸€ä¸ªé¢„è®­ç»ƒçš„embedding vectoråˆå§‹åŒ–ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Glove-50æ¥åˆå§‹åŒ–è¿™ä¸ªlayerï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸æ‰“ç®—è¿›ä¸€æ­¥æ›´æ–°/è®­ç»ƒè¿™ä¸ªlayerï¼Œå› ä¸ºæˆ‘ä»¬çš„è®­ç»ƒé›†å¤ªå°äº†ï¼Œæ›´æ–°embedding vectoråè€Œä¼šé€ æˆæ›´å·®çš„æ•ˆæœã€‚kerasçš„embedding layeræä¾›äº†æ§åˆ¶æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°embedding vectorçš„é€‰é¡¹

<img src="embedding1.png" style="width:700px;height:250px;">

å…¶ä¸­embedding layerè¾“å…¥çš„æœ€å¤§çš„æ•´æ•°ï¼ˆWord indexï¼‰åº”è¯¥ä¸å¤§äºå­—å…¸çš„å¤§å°ï¼Œembedding layerçš„è¾“å‡ºç»´åº¦ä¸º (batch size, max input length, dimension of word vectors).

æ‰€ä»¥ç¬¬ä¸€æ­¥åº”è¯¥æ˜¯å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºindexçš„æ•°ç»„ï¼Œç„¶åå¯¹æ ·æœ¬è¿›è¡Œzero-paddingä»¥å¯¹é½æœ€å¤§é•¿åº¦ã€‚

ä¸‹é¢çš„å‡½æ•°å®ç°äº†å°†è¾“å…¥çš„batch samplesè½¬æ¢ä¸ºbatch indexã€‚


```python
def sentences_to_indices(X, word_to_index, max_len):
    """
        å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºindexçš„å½¢å¼
    
        å‚æ•°ï¼š
            Xï¼šè¾“å…¥çš„å¥å­çš„æ•°ç»„ï¼Œå¤§å°ä¸º(m,1)
            word_to_indexï¼šWordåˆ°indexçš„æ˜ å°„
            max_len:å¯¹é½çš„æœ€å¤§é•¿åº¦
        è¿”å›ï¼š
            indexå½¢å¼çš„æ ·æœ¬ï¼Œå¤§å°ä¸º(m, max_len)
    """
    
    m = X.shape[0]
    
    X_indices = np.zeros((len(X), max_len))
    
    for i in range(m):
        
        sentence_words = X[i].lower().split()
        
        j = 0
        
        for w in sentence_words:

            X_indices[i, j] = word_to_index[w]
            j = j + 1
    
    return X_indices
```


```python
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    
    vocab_len = len(word_to_index)+1                  # è¿™æ˜¯kerasè¦æ±‚çš„+1
    #print (vocab_len)
    emb_dim = word_to_vec_map["cucumber"].shape[0]      # Word embeddingçš„é•¿åº¦
    
    emb_matrix = np.zeros((vocab_len, emb_dim))
    
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)

    embedding_layer.build((None,))
    
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```


```python
embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3])
```

    weights[0][1][3] = -0.3403


## 2.3 æ­å»ºæ¨¡å‹

ä¸‹å›¾ä¸­çš„æ¨¡å‹å°±æ˜¯æˆ‘ä»¬è¦æ­å»ºçš„RNNæ¨¡å‹ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬ç›´æ¥å°†embedding layerè¾“å‡ºä½œä¸ºLSTMç½‘ç»œçš„è¾“å…¥ï¼Œå¹¶ä¸”åªåœ¨æœ€åä¸€ä¸ªtime stepè¾“å‡ºï¼Œè¾“å‡ºåŒæ ·ä¸ºsoftmaxï¼Œè¾“å‡ºçš„ç»´åº¦ä¸º(m, 5)ã€‚å¹¶ä¸”ä¸ºäº†æ›´å¤šçš„æ¨¡å‹å¤æ‚åº¦ï¼Œå¦‚ä¸‹çš„ç½‘ç»œé‡‡ç”¨deep RNNè¿›è¡Œäº†å †å ï¼Œå¹¶ä¸”åœ¨å„å±‚ä¹‹é—´åŠ å…¥äº†dropoutæ¥å‡å°è¿‡æ‹Ÿåˆçš„é£é™©ã€‚

<img src="emojifier-v2.png" style="width:700px;height:400px;">


```python
def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
    # å®šä¹‰æ¨¡å‹çš„è¾“å…¥å±‚ï¼Œè¯¥è¾“å…¥çš„å¤§å°ä¸º(m, max_len)ï¼Œè¾“å…¥çš„å†…å®¹æ˜¯ç»è¿‡äº†indexè½¬æ¢çš„æ ·æœ¬æ•°æ®
    sentence_indices = Input(shape=input_shape, dtype='int32')
    
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # å°†embedding layeré“¾å¦‚æ¨¡å‹
    embeddings = embedding_layer(sentence_indices)   
    
    # æ¥ä¸‹æ¥æ­å»ºç¬¬ä¸€å±‚LSTMï¼Œè¿™ä¸€å±‚çš„éšè—å•å…ƒå¤§å°ä¸º128ï¼Œè€Œä¸”æ³¨æ„return_sequenceså‚æ•°è¡¨æ˜ç°åœ¨æ˜¯è¦è¿”å›æ•´ä¸ªsequenceçš„è¾“å‡ºï¼ˆTrueï¼‰è¿˜æ˜¯åªè¿”å›æœ€åä¸€ä¸ª
    # time stepçš„è¾“å‡ºï¼ˆFalseï¼‰ã€‚è¿™é‡Œæˆ‘ä»¬çš„ç¬¬ä¸€å±‚LSTMæ˜¯è¦è¿”å›æ•´ä¸ªåºåˆ—çš„è¾“å‡ºï¼Œæ‰€ä»¥ä¸ºTrueã€‚
    X = LSTM(units=128, return_sequences=True)(embeddings)
    X = Dropout(0.5)(X)
    # æ­å»ºç¬¬äºŒå±‚çš„LSTMï¼Œæ³¨æ„è¿™é‡Œæˆ‘ä»¬åªéœ€è¦è¿”å›æœ€åä¸€ä¸ªtimestepçš„è¾“å‡º
    X = LSTM(units=128)(X)
    X = Dropout(0.5)(X)
    X = Dense(5)(X)
    #X = softmax(X)
    X = Activation('softmax')(X)
    model = Model(inputs=sentence_indices, outputs=X)
    
    return model
```

è®°ä¸‹æ¥æˆ‘ä»¬è¿è¡Œä¸Šé¢çš„å‡½æ•°æ¥æ­å»ºæ¨¡å‹ã€‚ç”±äºæ•°æ®é›†ä¸­çš„æ ·æœ¬é•¿åº¦å‡å°äº10ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„max_lenå°±é€‰æ‹©10.

ç„¶åæˆ‘ä»¬æ‰“å°æ¨¡å‹çš„summaryæ¥çœ‹çœ‹ï¼Œæ€»å…±å‚æ•°ä¸º20223927ï¼Œä½†æ˜¯å¯è®­ç»ƒçš„å‚æ•°ä»…ä»…ä¸º223887ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„embedding layerå°±å ç”¨äº†20000050ä¸ªå‚æ•°ã€‚


```python
model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_12 (InputLayer)        (None, 10)                0         
    _________________________________________________________________
    embedding_13 (Embedding)     (None, 10, 50)            20000050  
    _________________________________________________________________
    lstm_15 (LSTM)               (None, 10, 128)           91648     
    _________________________________________________________________
    dropout_15 (Dropout)         (None, 10, 128)           0         
    _________________________________________________________________
    lstm_16 (LSTM)               (None, 128)               131584    
    _________________________________________________________________
    dropout_16 (Dropout)         (None, 128)               0         
    _________________________________________________________________
    dense_12 (Dense)             (None, 5)                 645       
    _________________________________________________________________
    activation_12 (Activation)   (None, 5)                 0         
    =================================================================
    Total params: 20,223,927
    Trainable params: 223,877
    Non-trainable params: 20,000,050
    _________________________________________________________________


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¦compileæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå› ä¸ºè¾“å‡ºä¸ºsoftmaxæ‰€ä»¥lossé€‰æ‹©categorical_crossentropyã€‚ç„¶åoptimizeré‡‡ç”¨æ¨¡å‹çš„Adamé…ç½®ï¼Œè¡¡é‡æ ‡å‡†é‡‡ç”¨accuracyï¼Œä»£è¡¨é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚


```python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```


```python
X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)
Y_train_oh = convert_to_one_hot(Y_train, C = 5)
```

ä¸‹é¢è®­ç»ƒçš„æ—¶å€™æˆ‘å¤§æ¦‚ç”¨äº†94ä¸ªepoch


```python
model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)
##validation_split=0.2, 
##initial_epoch=0
```

    Epoch 1/50
    132/132 [==============================] - 2s 15ms/step - loss: 1.5884 - acc: 0.2803
    Epoch 2/50
    132/132 [==============================] - 0s 713us/step - loss: 1.5250 - acc: 0.2955
    Epoch 3/50
    132/132 [==============================] - 0s 759us/step - loss: 1.4938 - acc: 0.3333
    Epoch 4/50
    132/132 [==============================] - 0s 1ms/step - loss: 1.4549 - acc: 0.3333
    Epoch 5/50
    132/132 [==============================] - 0s 1ms/step - loss: 1.3635 - acc: 0.3561
    Epoch 6/50
    132/132 [==============================] - 0s 1ms/step - loss: 1.2468 - acc: 0.4470
    Epoch 7/50
    132/132 [==============================] - 0s 913us/step - loss: 1.1242 - acc: 0.5606
    Epoch 8/50
    132/132 [==============================] - 0s 861us/step - loss: 1.0031 - acc: 0.6212
    Epoch 9/50
    132/132 [==============================] - 0s 755us/step - loss: 0.9289 - acc: 0.6515
    Epoch 10/50
    132/132 [==============================] - 0s 773us/step - loss: 0.8107 - acc: 0.7121
    Epoch 11/50
    132/132 [==============================] - 0s 796us/step - loss: 0.7050 - acc: 0.7576
    Epoch 12/50
    132/132 [==============================] - 0s 805us/step - loss: 0.7044 - acc: 0.7197
    Epoch 13/50
    132/132 [==============================] - 0s 778us/step - loss: 0.5743 - acc: 0.7803
    Epoch 14/50
    132/132 [==============================] - 0s 751us/step - loss: 0.6373 - acc: 0.8106
    Epoch 15/50
    132/132 [==============================] - 0s 809us/step - loss: 0.5212 - acc: 0.8333
    Epoch 16/50
    132/132 [==============================] - 0s 944us/step - loss: 0.3922 - acc: 0.8864
    Epoch 17/50
    132/132 [==============================] - 0s 865us/step - loss: 0.2963 - acc: 0.9318
    Epoch 18/50
    132/132 [==============================] - 0s 751us/step - loss: 0.3606 - acc: 0.8864
    Epoch 19/50
    132/132 [==============================] - 0s 749us/step - loss: 0.2566 - acc: 0.8864
    Epoch 20/50
    132/132 [==============================] - 0s 796us/step - loss: 0.2273 - acc: 0.9470
    Epoch 21/50
    132/132 [==============================] - 0s 832us/step - loss: 0.2383 - acc: 0.9242
    Epoch 22/50
    132/132 [==============================] - 0s 833us/step - loss: 0.3818 - acc: 0.8561
    Epoch 23/50
    132/132 [==============================] - 0s 946us/step - loss: 0.2569 - acc: 0.9242
    Epoch 24/50
    132/132 [==============================] - 0s 874us/step - loss: 0.1739 - acc: 0.9394
    Epoch 25/50
    132/132 [==============================] - 0s 780us/step - loss: 0.1507 - acc: 0.9621
    Epoch 26/50
    132/132 [==============================] - 0s 831us/step - loss: 0.1411 - acc: 0.9697
    Epoch 27/50
    132/132 [==============================] - 0s 854us/step - loss: 0.1459 - acc: 0.9545
    Epoch 28/50
    132/132 [==============================] - 0s 805us/step - loss: 0.1221 - acc: 0.9545
    Epoch 29/50
    132/132 [==============================] - 0s 843us/step - loss: 0.0743 - acc: 0.9848
    Epoch 30/50
    132/132 [==============================] - 0s 810us/step - loss: 0.0668 - acc: 0.9773
    Epoch 31/50
    132/132 [==============================] - 0s 816us/step - loss: 0.0787 - acc: 0.9697
    Epoch 32/50
    132/132 [==============================] - 0s 785us/step - loss: 0.0627 - acc: 0.9773
    Epoch 33/50
    132/132 [==============================] - 0s 832us/step - loss: 0.0682 - acc: 0.9773
    Epoch 34/50
    132/132 [==============================] - 0s 870us/step - loss: 0.0779 - acc: 0.9697
    Epoch 35/50
    132/132 [==============================] - 0s 793us/step - loss: 0.1150 - acc: 0.9697
    Epoch 36/50
    132/132 [==============================] - 0s 808us/step - loss: 0.0624 - acc: 0.9848
    Epoch 37/50
    132/132 [==============================] - 0s 773us/step - loss: 0.0370 - acc: 0.9848
    Epoch 38/50
    132/132 [==============================] - 0s 789us/step - loss: 0.0155 - acc: 1.0000
    Epoch 39/50
    132/132 [==============================] - 0s 855us/step - loss: 0.0293 - acc: 0.9848
    Epoch 40/50
    132/132 [==============================] - 0s 1ms/step - loss: 0.0271 - acc: 0.9924
    Epoch 41/50
    132/132 [==============================] - 0s 789us/step - loss: 0.0212 - acc: 1.0000
    Epoch 42/50
    132/132 [==============================] - 0s 836us/step - loss: 0.0142 - acc: 0.9924
    Epoch 43/50
    132/132 [==============================] - 0s 757us/step - loss: 0.0217 - acc: 1.0000
    Epoch 44/50
    132/132 [==============================] - 0s 830us/step - loss: 0.0200 - acc: 0.9924
    Epoch 45/50
    132/132 [==============================] - 0s 822us/step - loss: 0.0147 - acc: 1.0000
    Epoch 46/50
    132/132 [==============================] - 0s 853us/step - loss: 0.0156 - acc: 0.9924
    Epoch 47/50
    132/132 [==============================] - 0s 822us/step - loss: 0.0116 - acc: 1.0000
    Epoch 48/50
    132/132 [==============================] - 0s 747us/step - loss: 0.0042 - acc: 1.0000
    Epoch 49/50
    132/132 [==============================] - 0s 742us/step - loss: 0.0129 - acc: 1.0000
    Epoch 50/50
    132/132 [==============================] - 0s 801us/step - loss: 0.0166 - acc: 0.9924





    <keras.callbacks.History at 0x7f1adbab86a0>



ç”±äºæœ¬é¡¹ç›®çš„æ ·æœ¬æ˜¯åœ¨å¤ªå°‘ï¼Œæ‰€ä»¥æˆ‘åœ¨fitçš„æ—¶å€™å¹¶æ²¡æœ‰åˆ’åˆ†éªŒè¯é›†ç”¨äºæŸ¥çœ‹éªŒè¯é›†çš„å‡†ç¡®ç‡ã€‚æ€»å…±æ‰100å¤šä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†åˆ†é…å¤šäº†ï¼Œæ¨¡å‹è‚¯å®šè¿‡æ‹Ÿåˆï¼ŒéªŒè¯é›†åˆ†é…å°‘äº†ï¼Œé‚£éªŒè¯çš„ç»“æœåˆä¸å…·æœ‰ä»£è¡¨æ€§ï¼Œæ‰€ä»¥è¿™é‡Œå°±ä¸çº ç»“äºè°ƒå‚çš„è¿‡ç¨‹äº†ã€‚è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦ç›®çš„è¿˜æ˜¯ä¸ºäº†å­¦ä¹ RNNè¿˜æœ‰Word embeddingç­‰æ¦‚å¿µã€‚


```python
X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)
Y_test_oh = convert_to_one_hot(Y_test, C = 5)
loss, acc = model.evaluate(X_test_indices, Y_test_oh)
print()
print("Test accuracy = ", acc)
```

    56/56 [==============================] - 1s 12ms/step
    
    Test accuracy =  0.8392857142857143



```python
# ä¸‹é¢æŸ¥çœ‹åˆ’åˆ†é”™è¯¯çš„å¥å­
C = 5
y_test_oh = np.eye(C)[Y_test.reshape(-1)]
X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)
pred = model.predict(X_test_indices)
for i in range(len(X_test)):
    x = X_test_indices
    num = np.argmax(pred[i])
    if(num != Y_test[i]):
        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())
```

    Expected emoji:ğŸ˜„ prediction: she got me a nice present	â¤ï¸
    Expected emoji:ğŸ˜ prediction: work is hard	ğŸ˜„
    Expected emoji:ğŸ˜ prediction: This girl is messing with me	â¤ï¸
    Expected emoji:ğŸ˜ prediction: work is horrible	ğŸ˜„
    Expected emoji:ğŸ˜„ prediction: you brighten my day	â¤ï¸
    Expected emoji:ğŸ˜ prediction: she is a bully	ğŸ˜„
    Expected emoji:ğŸ˜ prediction: My life is so boring	â¤ï¸
    Expected emoji:ğŸ˜ prediction: I do not want to joke	â¤ï¸
    Expected emoji:ğŸ˜ prediction: go away	âš¾


åˆæ­¥åŸå› åˆ†æè¿˜æ˜¯è®­ç»ƒé›†çš„æ ·æœ¬å¤ªå°‘ï¼Œè™½ç„¶Word embeddingæ­å»ºçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¥½ï¼Œä½†æ˜¯å¦‚æœç±»ä¼¼çš„æ ·æœ¬ä¸€æ¬¡éƒ½æ²¡è§è¿‡çš„è¯ï¼Œé‚£ä¹ˆä¹Ÿæ˜¯æ— æ³•è¿›è¡Œæ³›åŒ–çš„ã€‚

ä¸‹é¢å°è¯•ä¸€ä¸‹è‡ªå·±æƒ³è¾“å…¥çš„å¥å­


```python
x_test = np.array(['not feeling happy'])
X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)
print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))
```

    not feeling happy ğŸ˜


ä¹‹å‰æˆ‘ä»¬ç®€å•çš„æ¨¡å‹æ— æ³•åˆ†è¾¨å‡ºnot feeling happy,ä½†æ˜¯ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥äº†ã€‚ä½†æ˜¯å½“å‰æ¨¡å‹ä¸èƒ½åˆ†è¾¨not happyï¼Œæ˜¯å› ä¸ºè´Ÿé¢æƒ…ç»ªçš„æ ·æœ¬å¤ªå°‘äº†ã€‚å¦‚æœè®­ç»ƒé›†æ›´åºå¤§çš„è¯é‚£ä¹ˆè¯¥æ¨¡å‹è‚¯å®šä¼šè¡¨ç°æ›´å‡ºè‰²ã€‚
